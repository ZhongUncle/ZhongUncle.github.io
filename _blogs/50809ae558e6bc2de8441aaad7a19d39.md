---
layout: article
category: Research
date: 2023-02-17
title: 在笔记本和 Mac mini 2018 上 Pytorch 实现的简单 NN 性能测试 - ZhongUncle
---
<!-- excerpt-start -->
测试了一下目前笔记本和 Mac mini 运行使用 Pytorch 构建的最简单的单层 NN （带有反向传播）的 10 次迭代，这样以后购买设备就知道需要注重哪些参数了，可惜没有。

首先是各个设备的性能：

| |Dell Inspiron 7490|Mac mini 2018|
|-|-|-|
|CPU|i7 10510U 4C8T 2.30-4.90GHz|i5 8500B 6C6T 3.00-4.10GHz|
|Cache|8 MB|9MB|
|内存|16GB DDR3 2133MHz|32GB DDR4 2666MHz|
|最优 GPU|MX250 4SM 384 CUDA Core 64-bit 2GB GDDR5|Intel® UHD Graphics 630 1.10 GHz|

下面结果取最好/中间值，唯一可惜的是 iMac 5K 卖早了，不然可以测一下 mps 加速的结果。因为 Python 的 mps 加速只在苹果芯片和搭载了 AMD 显卡的设备上支持。

|Dell Inspiron 7490 CPU|Dell Inspiron 7490 GPU|Mac mini 2018|
|-|-|-|
|123.15s|107.79s|87.94s|

最神奇的是 8500B 居然比 MX250 CUDA 加速之后的时间还短，当然这里很大概率是系统的问题，毕竟 Windows 性能表现确实不太行。

此外，显存带宽也真的很重要（虽然大小也重要），MX250 带宽太小，毕竟就 64-bit 的 GDDR5（也就是48GB/s），加迭代和降低学习速度等操作之后，CUDA 计算速率都上不去。不过一般消费级显卡的显存带宽也够用，只是 MX250 定位是轻薄本用。
这让我想到了我之前看别人做的测试 [Running PyTorch on the M1 GPU](https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html)，其中有个数据是 RTX 3080 Laptop 要比 RTX 2080Ti 稍慢一些：

<img src="/assets/images/vgg-benchmark-training.png" style="box-shadow: 0px 0px 0px 0px">

上图中各显卡参数和成绩如下：

| |GTX 1080Ti|Titan V|RTX 2080Ti|RTX 3060 Laptop|RTX 3080 Laptop|
|-|-|-|-|-|-|
|Cuda Cores|3584|5120|4352|3840|6144|
|显存接口|352-bit|3072-bit|352-bit|192-bit|256-bit|
|显存速率（Gbps）|11|1.7|14|14|14|
|显存带宽（GB/s）|484|652.8|616|336|448|
|分钟/epoch|7.65|5.03|5.75|14.53|6.66|

因为显存大小都为 10 GiB 左右，就不列出了，而且这里也没有超过显存大小。

<img src="/assets/images/%E6%88%AA%E5%B1%8F2023-02-17%2006.06.51.png" style="box-shadow: 0px 0px 0px 0px">


下图中“比例”部分关于“分钟/epoch”使用标准性能计算公式（如果你想质疑这个公式麻烦写论文去发 JACM）:

<img src="/assets/images/t27e187te8712.png" style="box-shadow: 0px 0px 0px 0px">

这样看可能不是很清楚，所以只保留“Cuda Cores”、“显存带宽”和“性能”来一张图，因为其他部分也都是与“显存带宽”相关的，所以就忽略掉：

<img src="/assets/images/%E6%88%AA%E5%B1%8F2023-02-17%2006.06.39.png" style="box-shadow: 0px 0px 0px 0px">

这样可以很清楚的看到，训练模型的性能和显存带宽、Cuda 数量的关系非常大。

当然，这里的显存带宽和性能是直接相关的，而 Cuda 数量则不一定。因为上面除了 GTX 1080 Ti 之外，都含有专门用来计算张量的 Tensor Core，所以要看具体实现。现在看 Tensor Core 的性能比较多，不过一般 Tensor Core 数量与 Cuda Core 数量挂钩。

当然如果你能直接找到相关 GPU 的浮点性能就更好了，这个更直接一些。研究这些是因为消费级 GPU 的浮点性能不是很好找。